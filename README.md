# ACTION-RECOGNITION-IN-SIGN-LANGUAGE-DETECTION-USING-LSTM

The implementation process of sign language recognition approach  involves several steps. Necessary libraries and dependencies are imported, including scikit-learn metrics for evaluation, OpenCV for image processing, NumPy for array operations, and MediaPipe for holistic model detection. Functions like mp_holistic=mp.solutions.holistic ,mp_drawing=mp.solutions.drawing_utils are used for performing MediaPipe detections, drawing landmarks on images, and extracting keypoints from the detected landmarks.
sklearn train_test_split function is used to split the dataset into training and testing sets.  An LSTM model is created using the TensorFlow Keras API. The model architecture consists of multiple LSTM layers followed by dense layers with ReLU activation functions. The model is compiled with the Adam optimizer and trained on the training dataset.
Model is trained on custom Dataset which consist of 40 word lexicon. Frames are captured from the webcam and uses MediaPipe to detect landmarks in each frame, and the extracted keypoints are saved in NumPy arrays. The process is repeated for multiple actions and sequences.
The trained model is loaded , and real-time action recognition is performed. The frames are captured from the webcam and uses MediaPipe to detect landmarks. The extracted keypoints are passed to the model for prediction. The predicted action label is determined based on the highest probability output from the model. The recognized actions are displayed on the screen.
